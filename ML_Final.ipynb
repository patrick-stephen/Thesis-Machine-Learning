{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d225e3-9429-418f-915b-552c01d17e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.ndimage import convolve\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, MaxPool2D, Flatten, Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8ff480-bc45-4834-a07a-accc8e09a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h5 to png images\n",
    "db_path = '/Users/Patrick/Downloads/LG36/r0087_2000.h5' # Path to the HDF5 file\n",
    "db = h5py.File(db_path, 'r')                            # Open the HDF5 file in read mode\n",
    "\n",
    "id = 0\n",
    "for key in db['data'].keys():                          # Iterating over the keys in 'data' group\n",
    "  img_datas = db['data'][key]['images']                # Accessing the 'images' dataset for the current key\n",
    "  for i in range(img_datas.shape[0]):                  # Iterating over each image in the dataset\n",
    "    id += 1                                            # Incrementing the image ID\n",
    "    img = img_datas[i]                                 # Extracting the current image\n",
    "    img[img < 0] = 0                                   # Replacing negative values with 0\n",
    "    pil_img = Image.fromarray(img, mode='I')           # Creating a PIL image from the array\n",
    "    pil_img = pil_img.resize((960, 960), Image.Resampling.BILINEAR) # # Resizing the image (Needed for Pillow 10)\n",
    "    img_name = '{:05d}.png'.format(id)                 # Formatting the image name\n",
    "    img_path = os.path.join('/Users/Patrick/Downloads/LG36/raw_images', img_name) # Creating the image path\n",
    "    pil_img.save(img_path)                             # Saving the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849d2526-7249-40cf-abe5-85d3896108c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing function for global normalization and then local normalization and then resampling to 300x300\n",
    "# and then cropping to center 100x100\n",
    "def preprocess(path):\n",
    "    image = Image.open(path)            # Opening the image from the given path\n",
    "    image_array = np.array(image)       # Converting the image to a numpy array\n",
    "    mean_value = np.mean(image_array)   # Calculating the global mean\n",
    "    std_value = np.std(image_array)     # Calculating the global standard deviation\n",
    "    normalized_image_global = (image_array - mean_value) / std_value # Applying global normalization\n",
    "    kernel = np.ones((3, 3)) / (3 ** 2) # Creating a kernel for local normalization\n",
    "    local_mean = convolve(normalized_image_global, kernel, mode='constant') # Local mean calculation\n",
    "    local_std = np.sqrt(convolve(np.square(normalized_image_global - local_mean), kernel, mode='constant') - local_mean ** 2) # Local std calculation\n",
    "    normalized_image_local = (normalized_image_global - local_mean) / local_std # Applying local normalization\n",
    "    normalized_image_local_display = np.clip(normalized_image_local, 0, 1) # Clipping values to [0, 1] range\n",
    "    normalized_image_local_display = Image.fromarray((normalized_image_local_display * 255)) # Converting array to PIL image\n",
    "    def crop_to_middle(img):\n",
    "        width, height = img.size    # Getting the image dimensions\n",
    "        left = (width - 100) / 2    # Calculating left coordinate for cropping\n",
    "        top = (height - 100) / 2    # Calculating top coordinate for cropping\n",
    "        right = (width + 100) / 2   # Calculating right coordinate for cropping\n",
    "        bottom = (height + 100) / 2 # Calculating bottom coordinate for cropping\n",
    "        cropped_img = img.crop((left, top, right, bottom)) # Cropping the image\n",
    "        return cropped_img\n",
    "    return crop_to_middle(normalized_image_local_display.resize((300, 300), Image.Resampling.NEAREST)).convert(\"L\") # Resizing, cropping, converting to grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6cb399-16e1-4b97-9133-725c7769c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying preprocessing to all images\n",
    "file_names = [f\"C:\\\\Users\\\\user\\\\Desktop\\\\Mnist_project\\\\dataset\\\\LG36_images\\\\{i:05d}.png\" for i in range(1, 2001)] # Creating file names for images\n",
    "for i in range(1, 2001):                                 # Iterating over image indices\n",
    "    preprocessed_image = preprocess(file_names[i-1])     # Preprocessing each image\n",
    "    preprocessed_image.save(f\"C:\\\\Users\\\\user\\\\Desktop\\\\Mnist_project\\\\dataset\\\\LG36_Preprocessed\\\\{i:05d}.png\") # Saving the preprocessed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a3aad1-4d2f-4560-8f95-f89ed8382564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn model creation\n",
    "def create_cnn_model(input_shape, num_classes):                       # Defining the function to create the CNN model\n",
    "    model = Sequential()                                              # Initializing a Sequential model\n",
    "    model.add(Conv2D(filters=3, kernel_size=(5,5), padding='valid', input_shape=(100, 100, 1))) # Adding a Conv2D layer\n",
    "    model.add(Activation('relu'))                                     # Adding a ReLU activation layer\n",
    "    model.add(MaxPool2D(strides=2, padding='same'))                   # Adding a MaxPooling layer\n",
    "\n",
    "    model.add(Conv2D(filters=3, kernel_size=(5,5), padding='valid'))  # Adding another Conv2D layer\n",
    "    model.add(Activation('relu'))                                     # Adding another ReLU activation layer\n",
    "    model.add(MaxPool2D(strides=2, padding='same'))                   # Adding another MaxPooling layer\n",
    "\n",
    "    model.add(Conv2D(filters=3, kernel_size=(5,5), padding='valid'))  # Adding another Conv2D layer\n",
    "    model.add(Activation('relu'))                                     # Adding another ReLU activation layer\n",
    "    model.add(MaxPool2D(strides=2, padding='same'))                   # Adding another MaxPooling layer\n",
    "    \n",
    "    model.add(Conv2D(filters=3, kernel_size=(5,5), padding='valid'))  # Adding another Conv2D layer\n",
    "    model.add(Activation('relu'))                                     # Adding another ReLU activation layer\n",
    "\n",
    "    model.add(Flatten())                                              # Adding a Flatten layer to convert 2D to 1D\n",
    "    model.add(Dense(3))                                               # Adding a Dense layer with 3 units\n",
    "    model.add(Activation('softmax'))                                  # Adding a softmax activation layer for classification\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8294957-0e50-40b5-8014-d72958d28dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring preprocessed images into memory\n",
    "preprocessed_images = [np.array(Image.open(path)) for path in [f\"C:\\\\Users\\\\user\\\\Desktop\\\\Mnist_project\\\\dataset\\\\LG36_Preprocessed\\\\{i:05d}.png\" for i in range(1, 2001)]] # Loading preprocessed images\n",
    "preprocessed_images = np.array(preprocessed_images)                  # Converting list of images to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb495cd-3dca-4888-994b-75adaf6fa453",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_display = np.squeeze(preprocessed_images[1750])             # Selecting an image to display\n",
    "plt.figure(figsize=(12, 6))                                          # Setting the figure size for the plot\n",
    "plt.imshow(image_to_display, cmap='gray')                            # Displaying the image in grayscale\n",
    "plt.show()                                                           # Showing the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5e09a5-6b66-47b2-b896-3ecee40b1019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in label file\n",
    "label_file='C:\\\\Users\\\\user\\\\Desktop\\\\Mnist_project\\\\dataset\\\\LG36.txt'   # Path to the label file\n",
    "labels = np.loadtxt(label_file, dtype='str', usecols=(2,), skiprows=0)    # Loading labels from the file\n",
    "label_mapping = {'HIT': 0, 'MAYBE': 1, 'MISS': 2}                         # Defining the label mapping\n",
    "numeric_labels = np.array([label_mapping[label] for label in labels])     # Converting labels to numeric form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b672f641-82de-4247-a2e1-ddbaf28d2f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_images, numeric_labels, test_size=0.2, random_state=42) # Splitting the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877ca841-abbd-4745-b883-c12bad5711d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (100, 100, 1)                         # Defining the input shape for the model\n",
    "num_classes = 3                                     # Defining the number of classes\n",
    "model = create_cnn_model(input_shape, num_classes)  # Creating the CNN model\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=4e-4) # Initializing the Adam optimizer\n",
    "model.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'], optimizer=adam) # Compiling the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb4cee7-26bd-470e-95a3-d5a61fbb11e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=64)  # Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbbd6b9-2011-429b-8b6f-12596e86f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate the model on test dataset to determine generalization\n",
    "_, acc = model.evaluate(X_test, y_test, batch_size=64, verbose=0)\n",
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bffa8f-a374-4230-a346-0d5a618bf292",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('C:\\\\Users\\\\user\\\\Desktop\\\\Mnist_project\\\\dataset\\\\TinyM.h5') # Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89188359-9bec-4b11-afe2-e280d67c8d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(X_test)        \n",
    "y_predict = y_predict.argmax(axis=1)     \n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots()\n",
    "cax = ax.matshow(cm, cmap=plt.cm.Blues)  # Use a color map that is visually distinct\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar(cax)\n",
    "\n",
    "# Set labels for axes\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "# Add class labels if necessary\n",
    "classes = ['HIT', 'MISS', 'MAYBE']  # Adjust as per your classes\n",
    "ax.set_xticklabels([''] + classes)\n",
    "ax.set_yticklabels([''] + classes)\n",
    "\n",
    "# Rotate the tick labels for aesthetics\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Annotate each cell with the numeric value using white or black depending on the background\n",
    "thresh = cm.max() / 2.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "# Save the figure to a file\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c524a9c4-1c30-4b12-bcea-41942371a202",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "plt.imshow(Image.open('confusion_matrix.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d6f619-b122-4d38-b3c5-19a0dc0d6ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
